{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interracial-station",
   "metadata": {},
   "source": [
    "# Modelos b√°sicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unknown-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import spacy\n",
    "from utils import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from torchtext.data import Field\n",
    "from torchtext.vocab import GloVe, Vectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-conjunction",
   "metadata": {},
   "source": [
    "Ver [Trained Models & Pipelines](https://spacy.io/models) para los modelos de SpaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suburban-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_md')\n",
    "\n",
    "def tokenize_en(sentence):\n",
    "    return [tok.text for tok in en.tokenizer(sentence)]\n",
    "\n",
    "es = spacy.load('es_core_news_md')\n",
    "\n",
    "def tokenize_es(sentence):\n",
    "    return [tok.text for tok in es.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "starting-faith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.vocab.GloVe at 0x7f742c378dd8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargamos los embeddings\n",
    "GloVe(name='twitter.27B', dim=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-hybrid",
   "metadata": {},
   "source": [
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pending-bronze",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>1</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>she calls herself \" anti - feminazi \" how abou...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>2</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>now , back to these women , the brave and the ...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>3</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@ curvybandida @ lynne _ i wow , your skirt is...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>objectification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>4</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@ aurelieguiboud incredible ! beautiful ! but ...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>5</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>i find it extremely hard to believe that kelly...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_case  id   source language  \\\n",
       "0  EXIST2021   1  twitter       en   \n",
       "1  EXIST2021   2  twitter       en   \n",
       "2  EXIST2021   3  twitter       en   \n",
       "3  EXIST2021   4  twitter       en   \n",
       "4  EXIST2021   5  twitter       en   \n",
       "\n",
       "                                                text       task1  \\\n",
       "0  she calls herself \" anti - feminazi \" how abou...      sexist   \n",
       "1  now , back to these women , the brave and the ...  non-sexist   \n",
       "2  @ curvybandida @ lynne _ i wow , your skirt is...      sexist   \n",
       "3  @ aurelieguiboud incredible ! beautiful ! but ...  non-sexist   \n",
       "4  i find it extremely hard to believe that kelly...  non-sexist   \n",
       "\n",
       "                    task2  \n",
       "0  ideological-inequality  \n",
       "1              non-sexist  \n",
       "2         objectification  \n",
       "3              non-sexist  \n",
       "4              non-sexist  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../../Data/EXIST2021_training.tsv', sep='\\t')\n",
    "# train = pd.read_csv('../../Data/EXIST2021_training_spell_checked.csv', sep=',')\n",
    "\n",
    "# Un simple pre-procesamiento\n",
    "train['text'] = train['text'].apply(lambda text: preprocessing.preprocess(text))\n",
    "\n",
    "train_en = train[train['language'] == 'en']\n",
    "train_es = train[train['language'] == 'es']\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "artificial-cement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>language</th>\n",
       "      <th>text</th>\n",
       "      <th>task1</th>\n",
       "      <th>task2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6978</td>\n",
       "      <td>gab</td>\n",
       "      <td>en</td>\n",
       "      <td>pennsylvania state rep horrifies with opening ...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6979</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@ iilovegrapes he sounds like as ass , and ver...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6980</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@ averyangryskel 1 @ 4arealistparty lol ! \" th...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6981</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>@ wanderorange @ stalliontwink rights ? i mean...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>ideological-inequality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXIST2021</td>\n",
       "      <td>6982</td>\n",
       "      <td>twitter</td>\n",
       "      <td>en</td>\n",
       "      <td>the jack manifold appreciation i'm seeing is o...</td>\n",
       "      <td>non-sexist</td>\n",
       "      <td>non-sexist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_case    id   source language  \\\n",
       "0  EXIST2021  6978      gab       en   \n",
       "1  EXIST2021  6979  twitter       en   \n",
       "2  EXIST2021  6980  twitter       en   \n",
       "3  EXIST2021  6981  twitter       en   \n",
       "4  EXIST2021  6982  twitter       en   \n",
       "\n",
       "                                                text       task1  \\\n",
       "0  pennsylvania state rep horrifies with opening ...  non-sexist   \n",
       "1  @ iilovegrapes he sounds like as ass , and ver...  non-sexist   \n",
       "2  @ averyangryskel 1 @ 4arealistparty lol ! \" th...      sexist   \n",
       "3  @ wanderorange @ stalliontwink rights ? i mean...      sexist   \n",
       "4  the jack manifold appreciation i'm seeing is o...  non-sexist   \n",
       "\n",
       "                    task2  \n",
       "0              non-sexist  \n",
       "1              non-sexist  \n",
       "2  ideological-inequality  \n",
       "3  ideological-inequality  \n",
       "4              non-sexist  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../../Data/EXIST2021_test.tsv', sep='\\t')\n",
    "# test = pd.read_csv('../../Data/EXIST2021_test_spell_checked.csv', sep=',')\n",
    "\n",
    "# Un simple pre-procesamiento\n",
    "test['text'] = test['text'].apply(lambda text: preprocessing.preprocess(text))\n",
    "\n",
    "test_en = test[test['language'] == 'en']\n",
    "test_es = test[test['language'] == 'es']\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "utility-gentleman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non-sexist', 'sexist'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train['task1'])\n",
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "comprehensive-meeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'incredible ! aaa :D beautiful ! but i laughed sooo much when i read about you drifting in your wheelchair . i can just picture it lol'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '@AurelieGuiboud Incredible! AAAA :D Beautiful!But I laughed sooooooo much when I read about you drifting in your wheelchair.I can just picture it  https://t.co/uvl5HhbmbR lol'\n",
    "preprocessing.preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-disabled",
   "metadata": {},
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-freedom",
   "metadata": {},
   "source": [
    "### Baseline (tf-idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-current",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "framed-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el vectorizer\n",
    "vectorizer_en = TfidfVectorizer(analyzer='word', stop_words=None, lowercase=True)\n",
    "vectorizer_en.fit(train_en['text'])\n",
    "\n",
    "# Transformamos\n",
    "X_train_en = vectorizer_en.transform(train_en['text'])\n",
    "X_test_en = vectorizer_en.transform(test_en['text'])\n",
    "\n",
    "y_train_en = label_encoder.transform(train_en['task1'])\n",
    "y_test_en = label_encoder.transform(test_en['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "introductory-nitrogen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 720 ms, sys: 2.11 s, total: 2.83 s\n",
      "Wall time: 190 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.67      0.74      0.70      1050\n",
      "      sexist       0.74      0.66      0.70      1158\n",
      "\n",
      "    accuracy                           0.70      2208\n",
      "   macro avg       0.70      0.70      0.70      2208\n",
      "weighted avg       0.71      0.70      0.70      2208\n",
      "\n",
      "Accuracy: 0.7011\n"
     ]
    }
   ],
   "source": [
    "clf_en = LogisticRegression(max_iter=1000)\n",
    "%time clf_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "y_pred_en = clf_en.predict(X_test_en)\n",
    "\n",
    "print(classification_report(y_test_en, y_pred_en, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_en, y_pred_en), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-registrar",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demanding-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el vectorizer\n",
    "vectorizer_es = TfidfVectorizer(analyzer='word', stop_words=None, lowercase=True)\n",
    "vectorizer_es.fit(train_es['text'])\n",
    "\n",
    "# Transformamos\n",
    "X_train_es = vectorizer_es.transform(train_es['text'])\n",
    "X_test_es = vectorizer_es.transform(test_es['text'])\n",
    "\n",
    "y_train_es = label_encoder.transform(train_es['task1'])\n",
    "y_test_es = label_encoder.transform(test_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "forty-yahoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 482 ms, sys: 1.1 s, total: 1.59 s\n",
      "Wall time: 113 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.66      0.80      0.72      1037\n",
      "      sexist       0.77      0.62      0.69      1123\n",
      "\n",
      "    accuracy                           0.71      2160\n",
      "   macro avg       0.72      0.71      0.70      2160\n",
      "weighted avg       0.72      0.71      0.70      2160\n",
      "\n",
      "Accuracy: 0.706\n"
     ]
    }
   ],
   "source": [
    "clf_es = LogisticRegression(max_iter=1000)\n",
    "%time clf_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "y_pred_es = clf_es.predict(X_test_es)\n",
    "\n",
    "print(classification_report(y_test_es, y_pred_es, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_es, y_pred_es), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-republican",
   "metadata": {},
   "source": [
    "#### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "embedded-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.66      0.77      0.71      2087\n",
      "      sexist       0.75      0.64      0.69      2281\n",
      "\n",
      "    accuracy                           0.70      4368\n",
      "   macro avg       0.71      0.71      0.70      4368\n",
      "weighted avg       0.71      0.70      0.70      4368\n",
      "\n",
      "Accuracy: 0.7035\n"
     ]
    }
   ],
   "source": [
    "y_test = np.hstack((y_test_en, y_test_es))\n",
    "y_pred = np.hstack((y_pred_en, y_pred_es))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-fantasy",
   "metadata": {},
   "source": [
    "### Promedio de vectores de palabras con GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-public",
   "metadata": {},
   "source": [
    "Para los embeddings en ingl√©s usar√© los que est√°n pre-entrenados con el corpus de Twitter, pues los datos de esta tarea tambi√©n son de Twitter.\n",
    "\n",
    "Debido a que Torchtext s√≥lo tiene por defecto embeddings en ingl√©s hay que hacer otras cosas para cargar los que est√°n en espa√±ol. Primero, hay que descargarlos de [GloVe Spanish](http://dcc.uchile.cl/~jperez/word-embeddings/glove-sbwc.i25.vec.gz) y ponerlos en la carpeta `.vector_cache` del directorio actual. Para ver otros embeddings pre-entrenados en espa√±ol ver [spanish-word-embeddings](https://github.com/dccuchile/spanish-word-embeddings).\n",
    "\n",
    "Ver lo siguiente para algunos detalles de c√≥mo cargarlos para la capa de embeddings:\n",
    "\n",
    "- [Use pretrained embedding in Spanish with Torchtext](https://stackoverflow.com/questions/52224555/use-pretrained-embedding-in-spanish-with-torchtext)\n",
    "- [Handling German Text with torchtext](https://www.innoq.com/en/blog/handling-german-text-with-torchtext/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "independent-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(text, text_field, vocab):\n",
    "    \"\"\"\n",
    "    Promedia los vectores de palabras de un texto.\n",
    "    \"\"\"\n",
    "    vectors = np.array([vocab.vectors[vocab[token]].numpy() for token in text_field.preprocess(text)])\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-civilian",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spiritual-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documentos/PyTorch-1.x/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 6696),\n",
       " ('i', 5017),\n",
       " ('@', 4287),\n",
       " ('/', 3123),\n",
       " ('the', 2767),\n",
       " ('to', 2541),\n",
       " (',', 2518),\n",
       " ('a', 2438),\n",
       " ('and', 2033),\n",
       " ('#', 2022)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos c√≥mo se preprocesar√° el texto\n",
    "text_field_en = Field(tokenize=tokenize_en, lower=True)\n",
    "\n",
    "# Preprocesamos el texto\n",
    "preprocessed_train_text_en = train_en['text'].apply(lambda x: text_field_en.preprocess(x))\n",
    "preprocessed_test_text_en = test_en['text'].apply(lambda x: text_field_en.preprocess(x))\n",
    "\n",
    "# Contruimos el vocabulario\n",
    "text_field_en.build_vocab(preprocessed_train_text_en, vectors='glove.twitter.27B.200d', vectors_cache='.vector_cache')\n",
    "vocab_en = text_field_en.vocab\n",
    "vocab_en.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "earned-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3436/3436 [00:00<00:00, 5866.75it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_en = np.zeros(shape=(train_en.shape[0], 200))\n",
    "\n",
    "for i, text in tqdm(enumerate(train_en['text']), total=train_en.shape[0]):\n",
    "    X_train_en[i, :] = mean_vector(text, text_field_en, vocab_en)\n",
    "    \n",
    "y_train_en = label_encoder.transform(train_en['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "married-bobby",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2208/2208 [00:00<00:00, 5375.46it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_en = np.zeros(shape=(test_en.shape[0], 200))\n",
    "\n",
    "for i, text in tqdm(enumerate(test_en['text']), total=test_en.shape[0]):\n",
    "    X_test_en[i, :] = mean_vector(text, text_field_en, vocab_en)\n",
    "    \n",
    "y_test_en = label_encoder.transform(test_en['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "naughty-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 373 ms, sys: 804 ms, total: 1.18 s\n",
      "Wall time: 81.5 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.63      0.74      0.68      1050\n",
      "      sexist       0.72      0.60      0.65      1158\n",
      "\n",
      "    accuracy                           0.67      2208\n",
      "   macro avg       0.67      0.67      0.67      2208\n",
      "weighted avg       0.68      0.67      0.67      2208\n",
      "\n",
      "Accuracy: 0.6685\n"
     ]
    }
   ],
   "source": [
    "clf_en = LogisticRegression(max_iter=1000)\n",
    "%time clf_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "y_pred_en = clf_en.predict(X_test_en)\n",
    "\n",
    "print(classification_report(y_test_en, y_pred_en, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_en, y_pred_en), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-richmond",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "better-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_es = Vectors('glove-sbwc.i25.vec', cache='.vector_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collect-sodium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Documentos/PyTorch-1.x/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 6021),\n",
       " (',', 4472),\n",
       " ('que', 4010),\n",
       " ('de', 3845),\n",
       " ('@', 3629),\n",
       " ('/', 3172),\n",
       " ('la', 2813),\n",
       " ('a', 2683),\n",
       " ('y', 2679),\n",
       " ('no', 2171)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos c√≥mo se preprocesar√° el texto\n",
    "text_field_es = Field(tokenize=tokenize_es, lower=True)\n",
    "\n",
    "# Preprocesamos el texto\n",
    "preprocessed_train_text_es = train_es['text'].apply(lambda x: text_field_es.preprocess(x))\n",
    "preprocessed_test_text_es = test_es['text'].apply(lambda x: text_field_es.preprocess(x))\n",
    "\n",
    "# Contruimos el vocabulario\n",
    "text_field_es.build_vocab(preprocessed_train_text_es, vectors=vectors_es)\n",
    "vocab_es = text_field_es.vocab\n",
    "vocab_es.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "modified-elements",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3541/3541 [00:00<00:00, 6674.76it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_es = np.zeros(shape=(train_es.shape[0], 300))\n",
    "\n",
    "for i, text in tqdm(enumerate(train_es['text']), total=train_es.shape[0]):\n",
    "    X_train_es[i, :] = mean_vector(text, text_field_es, vocab_es)\n",
    "    \n",
    "y_train_es = label_encoder.transform(train_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "modified-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3541/3541 [00:00<00:00, 6725.38it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_es = np.zeros(shape=(train_es.shape[0], 300))\n",
    "\n",
    "for i, text in tqdm(enumerate(train_es['text']), total=train_es.shape[0]):\n",
    "    X_train_es[i, :] = mean_vector(text, text_field_es, vocab_es)\n",
    "    \n",
    "y_train_es = label_encoder.transform(train_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "virgin-carpet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2160/2160 [00:00<00:00, 6328.67it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_es = np.zeros(shape=(test_es.shape[0], 300))\n",
    "\n",
    "for i, text in tqdm(enumerate(test_es['text']), total=test_es.shape[0]):\n",
    "    X_test_es[i, :] = mean_vector(text, text_field_es, vocab_es)\n",
    "    \n",
    "y_test_es = label_encoder.transform(test_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "straight-independence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 570 ms, sys: 1.08 s, total: 1.65 s\n",
      "Wall time: 111 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.80      0.70      1037\n",
      "           1       0.75      0.56      0.64      1123\n",
      "\n",
      "    accuracy                           0.68      2160\n",
      "   macro avg       0.69      0.68      0.67      2160\n",
      "weighted avg       0.69      0.68      0.67      2160\n",
      "\n",
      "Accuracy: 0.6755\n"
     ]
    }
   ],
   "source": [
    "clf_es = LogisticRegression(max_iter=1000)\n",
    "%time clf_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "y_pred_es = clf_es.predict(X_test_es)\n",
    "\n",
    "print(classification_report(y_test_es, y_pred_es))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_es, y_pred_es), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-france",
   "metadata": {},
   "source": [
    "#### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "formal-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.63      0.77      0.69      2087\n",
      "      sexist       0.74      0.58      0.65      2281\n",
      "\n",
      "    accuracy                           0.67      4368\n",
      "   macro avg       0.68      0.68      0.67      4368\n",
      "weighted avg       0.68      0.67      0.67      4368\n",
      "\n",
      "Accuracy: 0.6719\n"
     ]
    }
   ],
   "source": [
    "y_test = np.hstack((y_test_en, y_test_es))\n",
    "y_pred = np.hstack((y_pred_en, y_pred_es))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-ranch",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-object",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-bahrain",
   "metadata": {},
   "source": [
    "Al igual que los vectores usados en GloVe har√© que los embeddings the Doc2Vec tengan dimesi√≥n igual a 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "statistical-garden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.6 s, sys: 1.76 s, total: 23.4 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "documents_en = [TaggedDocument(doc, [i]) for doc, i in zip(preprocessed_train_text_en, train_en['task1'])]\n",
    "%time doc2vec_en = Doc2Vec(documents_en, vector_size=200, window=5, min_count=1, workers=4, seed=42, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ideal-oakland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3436/3436 [00:08<00:00, 389.83it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_en = np.zeros(shape=(train_en.shape[0], 200))\n",
    "\n",
    "for i, text in tqdm(enumerate(train_en['text']), total=train_en.shape[0]):\n",
    "    X_train_en[i, :] = doc2vec_en.infer_vector(text_field_en.preprocess(text))\n",
    "    \n",
    "y_train_en = label_encoder.transform(train_en['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "terminal-white",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2208/2208 [00:05<00:00, 387.68it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_en = np.zeros(shape=(test_en.shape[0], 200))\n",
    "\n",
    "for i, text in tqdm(enumerate(test_en['text']), total=test_en.shape[0]):\n",
    "    X_test_en[i, :] = doc2vec_en.infer_vector(text_field_en.preprocess(text))\n",
    "    \n",
    "y_test_en = label_encoder.transform(test_en['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "suburban-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 173 ms, sys: 225 ms, total: 398 ms\n",
      "Wall time: 29.9 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.61      0.58      1050\n",
      "           1       0.60      0.53      0.57      1158\n",
      "\n",
      "    accuracy                           0.57      2208\n",
      "   macro avg       0.57      0.57      0.57      2208\n",
      "weighted avg       0.57      0.57      0.57      2208\n",
      "\n",
      "Accuracy: 0.5707\n"
     ]
    }
   ],
   "source": [
    "clf_en = LogisticRegression(max_iter=1000)\n",
    "%time clf_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "y_pred_en = clf_en.predict(X_test_en)\n",
    "\n",
    "print(classification_report(y_test_en, y_pred_en))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_en, y_pred_en), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-things",
   "metadata": {},
   "source": [
    "#### Spanish\n",
    "Al igual que los vectores usados en GloVe har√© que los embeddings the Doc2Vec tengan dimesi√≥n igual a 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "electric-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.8 s, sys: 1.76 s, total: 25.5 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "documents_es = [TaggedDocument(doc, [i]) for doc, i in zip(preprocessed_train_text_es, train_es['task1'])]\n",
    "%time doc2vec_es = Doc2Vec(documents_es, vector_size=300, window=5, min_count=1, workers=4, seed=42, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "christian-croatia",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3541/3541 [00:09<00:00, 366.71it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_es = np.zeros(shape=(train_es.shape[0], 300))\n",
    "\n",
    "for i, text in tqdm(enumerate(train_es['text']), total=train_es.shape[0]):\n",
    "    X_train_es[i, :] = doc2vec_es.infer_vector(text_field_es.preprocess(text))\n",
    "    \n",
    "y_train_es = label_encoder.transform(train_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "utility-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2160/2160 [00:05<00:00, 371.47it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_es = np.zeros(shape=(test_es.shape[0], 300))\n",
    "\n",
    "for i, text in tqdm(enumerate(test_es['text']), total=test_es.shape[0]):\n",
    "    X_test_es[i, :] = doc2vec_es.infer_vector(text_field_es.preprocess(text))\n",
    "    \n",
    "y_test_es = label_encoder.transform(test_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tropical-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 249 ms, sys: 385 ms, total: 634 ms\n",
      "Wall time: 44.3 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.58      0.57      1037\n",
      "           1       0.60      0.59      0.59      1123\n",
      "\n",
      "    accuracy                           0.58      2160\n",
      "   macro avg       0.58      0.58      0.58      2160\n",
      "weighted avg       0.58      0.58      0.58      2160\n",
      "\n",
      "Accuracy: 0.5829\n"
     ]
    }
   ],
   "source": [
    "clf_es = LogisticRegression(max_iter=1000)\n",
    "%time clf_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "y_pred_es = clf_es.predict(X_test_es)\n",
    "\n",
    "print(classification_report(y_test_es, y_pred_es))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_es, y_pred_es), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-arabic",
   "metadata": {},
   "source": [
    "#### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "mounted-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.55      0.60      0.57      2087\n",
      "      sexist       0.60      0.56      0.58      2281\n",
      "\n",
      "    accuracy                           0.58      4368\n",
      "   macro avg       0.58      0.58      0.58      4368\n",
      "weighted avg       0.58      0.58      0.58      4368\n",
      "\n",
      "Accuracy: 0.5767\n"
     ]
    }
   ],
   "source": [
    "y_test = np.hstack((y_test_en, y_test_es))\n",
    "y_pred = np.hstack((y_pred_en, y_pred_es))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-yesterday",
   "metadata": {},
   "source": [
    "### sentence-BERT\n",
    "\n",
    "En [Pretrained Models](https://www.sbert.net/docs/pretrained_models.html) est√°n todos los modelos preentrenados que hay. Los modelos fuertes en una tarea, ser√°n d√©biles para otra tarea, por lo tanto, es importante seleccionar el modelo adecuado para cada tarea. Como no hay ninguno en espec√≠fico para la tarea de an√°lisis de sentimientos usar√© **paraphrase-distilroberta-base-v1**, el cual recomiendan para varias aplicaciones.\n",
    "\n",
    "**distiluse-base-multilingual-cased-v1:** Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. Supports 15 languages: Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "addressed-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descagamos el modelo\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-effect",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "intellectual-scene",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_en = [' '.join(review) for review in preprocessed_train_text_en]\n",
    "sentences_test_en = [' '.join(review) for review in preprocessed_test_text_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "adverse-inclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad11dbeaef2c457196ec11146c183404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.43 s, sys: 603 ms, total: 7.04 s\n",
      "Wall time: 5.81 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6f189a609d405c9b0a313a2e29728b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.45 s, sys: 146 ms, total: 3.6 s\n",
      "Wall time: 2.76 s\n"
     ]
    }
   ],
   "source": [
    "# Cada embedding tiene dimensi√≥n igual a 768\n",
    "%time X_train_en = model.encode(sentences_train_en, show_progress_bar=True)\n",
    "%time X_test_en = model.encode(sentences_test_en, show_progress_bar=True)\n",
    "\n",
    "y_train_en = label_encoder.transform(train_en['task1'])\n",
    "y_test_en = label_encoder.transform(test_en['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "surgical-yahoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 155 ms, sys: 377 ms, total: 532 ms\n",
      "Wall time: 42.5 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.73      1050\n",
      "           1       0.76      0.71      0.73      1158\n",
      "\n",
      "    accuracy                           0.73      2208\n",
      "   macro avg       0.73      0.73      0.73      2208\n",
      "weighted avg       0.73      0.73      0.73      2208\n",
      "\n",
      "Accuracy: 0.7301\n"
     ]
    }
   ],
   "source": [
    "clf_en = LogisticRegression(max_iter=1000)\n",
    "%time clf_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "y_pred_en = clf_en.predict(X_test_en)\n",
    "\n",
    "print(classification_report(y_test_en, y_pred_en))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_en, y_pred_en), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-royal",
   "metadata": {},
   "source": [
    "#### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "otherwise-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train_es = [' '.join(review) for review in preprocessed_train_text_es]\n",
    "sentences_test_es = [' '.join(review) for review in preprocessed_test_text_es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "likely-liberia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1396eafd0a444346aa2832374e9fbeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/111 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.35 s, sys: 141 ms, total: 5.49 s\n",
      "Wall time: 4.2 s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b42dd876404ef48d19b41fd0c272cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.37 s, sys: 78.8 ms, total: 3.45 s\n",
      "Wall time: 2.65 s\n"
     ]
    }
   ],
   "source": [
    "# Cada embedding tiene dimensi√≥n igual a 768\n",
    "%time X_train_es = model.encode(sentences_train_es, show_progress_bar=True)\n",
    "%time X_test_es = model.encode(sentences_test_es, show_progress_bar=True)\n",
    "\n",
    "y_train_es = label_encoder.transform(train_es['task1'])\n",
    "y_test_es = label_encoder.transform(test_es['task1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "extraordinary-happening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 337 ms, sys: 592 ms, total: 930 ms\n",
      "Wall time: 68.8 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.79      0.73      1037\n",
      "           1       0.77      0.64      0.70      1123\n",
      "\n",
      "    accuracy                           0.71      2160\n",
      "   macro avg       0.72      0.72      0.71      2160\n",
      "weighted avg       0.72      0.71      0.71      2160\n",
      "\n",
      "Accuracy: 0.712\n"
     ]
    }
   ],
   "source": [
    "clf_es = LogisticRegression(max_iter=1000)\n",
    "%time clf_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "y_pred_es = clf_es.predict(X_test_es)\n",
    "\n",
    "print(classification_report(y_test_es, y_pred_es))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test_es, y_pred_es), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-broadcast",
   "metadata": {},
   "source": [
    "#### Total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "silent-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  non-sexist       0.68      0.77      0.73      2087\n",
      "      sexist       0.76      0.68      0.72      2281\n",
      "\n",
      "    accuracy                           0.72      4368\n",
      "   macro avg       0.72      0.72      0.72      4368\n",
      "weighted avg       0.73      0.72      0.72      4368\n",
      "\n",
      "Accuracy: 0.7212\n"
     ]
    }
   ],
   "source": [
    "y_test = np.hstack((y_test_en, y_test_es))\n",
    "y_pred = np.hstack((y_pred_en, y_pred_es))\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=['non-sexist', 'sexist']))\n",
    "print(f'Accuracy: {round(accuracy_score(y_test, y_pred), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-pencil",
   "metadata": {},
   "source": [
    "## Resultados\n",
    "\n",
    "De los distintos modelos el mejor parece ser el de BERT que ha sido entrenado con varios idiomas.\n",
    "\n",
    "- Al hacer la clasificaci√≥n sin hacer el spell-check se obtiene:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-7btt\" rowspan=\"2\">Model</th>\n",
    "    <th class=\"tg-7btt\" colspan=\"3\">Task-1 (accuracy)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">English</td>\n",
    "    <td class=\"tg-7btt\">Spanish</td>\n",
    "    <td class=\"tg-7btt\">Total</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">tf-idf</td>\n",
    "    <td class=\"tg-c3ow\">70.70</td>\n",
    "    <td class=\"tg-c3ow\">69.81</td>\n",
    "    <td class=\"tg-c3ow\">70.26</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">GloVe</td>\n",
    "    <td class=\"tg-c3ow\">66.35</td>\n",
    "    <td class=\"tg-c3ow\">67.82</td>\n",
    "    <td class=\"tg-c3ow\">67.08</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc2Vec</td>\n",
    "    <td class=\"tg-c3ow\">57.43</td>\n",
    "    <td class=\"tg-c3ow\">59.31</td>\n",
    "    <td class=\"tg-c3ow\">58.36</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">sentence-BERT</td>\n",
    "    <td class=\"tg-c3ow\">73.73</td>\n",
    "    <td class=\"tg-c3ow\">72.64</td>\n",
    "    <td class=\"tg-c3ow\">73.19</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "- Al hacer la clasificaci√≥n con el spell-check se obtiene:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{border-color:inherit;font-weight:bold;text-align:center;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-7btt\" rowspan=\"2\">Model</th>\n",
    "    <th class=\"tg-7btt\" colspan=\"3\">Task-1 (accuracy)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-7btt\">English</td>\n",
    "    <td class=\"tg-7btt\">Spanish</td>\n",
    "    <td class=\"tg-7btt\">Total</td>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">tf-idf</td>\n",
    "    <td class=\"tg-c3ow\">70.11</td>\n",
    "    <td class=\"tg-c3ow\">70.60</td>\n",
    "    <td class=\"tg-c3ow\">70.35</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">GloVe</td>\n",
    "    <td class=\"tg-c3ow\">66.85</td>\n",
    "    <td class=\"tg-c3ow\">67.55</td>\n",
    "    <td class=\"tg-c3ow\">67.19</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">Doc2Vec</td>\n",
    "    <td class=\"tg-c3ow\">57.07</td>\n",
    "    <td class=\"tg-c3ow\">58.29</td>\n",
    "    <td class=\"tg-c3ow\">57.67</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">sentence-BERT</td>\n",
    "    <td class=\"tg-c3ow\">73.01</td>\n",
    "    <td class=\"tg-c3ow\">71.20</td>\n",
    "    <td class=\"tg-c3ow\">72.12</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "En realidad parece empeorar un poco con el spell-check (principalmente en sentence-BERT que es el que mejor sale), adem√°s de que es algo tardado hacerlo, por lo que mejor no usar√© el spell-check. Por otro lado, hacer el pre-procesamiento parece ayudar un poco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-suggestion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
